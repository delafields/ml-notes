{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold and Other Cross-Validation Techniques\n",
    "\n",
    "In Machine Learning, we usually divide the dataset into a training set, a validation set, and a test set:\n",
    "* **Training set:** used to train the model - it can vary but it's typically 60-70% of the available data.\n",
    "* **Validation set:** once we select a model that performs well on the training data, we run the model on the validation set. The validation set helps to provide an uniased evaluation on the model's performance. If the error increases on the validation set then we have an overfitting model. This is typically 10-20% of the available data.\n",
    "* **Test set:** or **holdout set**, this data set contains data that has never been used in the training process and helps evaluate the final model. Typically 5-20% of the available data.\n",
    "![data split ex](https://cdn-images-1.medium.com/max/1200/1*v1kM3rIPwxTYyTk5eIunFg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the problem with just using a train/test split?\n",
    "Due to **sample variability** between the train and test set, our model can give a better prediction on the training data but fail to generalize on the test set (overfitting). This leads to a low training error rate but a high test error rate.\n",
    "\n",
    "When we split the dataset into a train/validation/test split, we only use a subset of the data and we know when we train on fewer observations the model will not perform well and **overestimate** the test error rate for the model to fit on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: Cross-Validation\n",
    "Cross-Validation is a technique which involves partitioning the data into subsets, training the data on a subset and using the other subset to evalutate the model's performance.\n",
    "\n",
    "To reduce variability, we perform multiple rounds of cross-validation with different subsets from the same data. We combine the validation results from these multiple rounds to come up with an estimate of the model's predictive performance.\n",
    "\n",
    "![cross val ex](https://cdn-images-1.medium.com/max/1200/1*sWQi89jsD84iYcWb4yBB3g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [[1 2]]\n",
      "validation: [[3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "train, validation = train_test_split(data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"train: {train}\")\n",
    "print(f\"validation: {validation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Types of Cross-Validation\n",
    "* LOOCV - Leave one out cross-validation\n",
    "* KFold\n",
    "* Stratified cross-validation\n",
    "* Time series cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one out cross-validation (LOOCV)\n",
    "In LOOCV, we divide the dataset into two parts\n",
    "1. A single observation which is our test data\n",
    "2. All other observations from the dataset, forming our training data\n",
    "\n",
    "If we have a dataset with $n$ observations, then the training data contains $n-1$ observations and the test set contains one observation. This process is iterated for each data point, processed $n$ times and generates $n$ times Mean Squared Error(MSE).\n",
    "\n",
    "![LOOCV ex](https://cdn-images-1.medium.com/max/1200/1*AVVhcmOs7WCBnpNhqi-L6g.png)\n",
    "\n",
    "##### Advantages\n",
    "* Far less bias as we have used the entire dataset for training compared to the validation set approach where we only use a subset of the data for training\n",
    "* No randoness in the train/test data as performing LOOCV multiple times will yield the same results\n",
    "##### Disadvantages\n",
    "* MSE will vary as test data uses a single observation, introducing variability. If the data point is an outlier than the variability will be much higher.\n",
    "* Execution is expensive as the model has to be fitted $n$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1], validation: [0]\n",
      "train: [0], validation: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([[1,2], [3,4]])\n",
    "y = np.array([1,2])\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(f\"train: {train_index}, validation: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold cross validation\n",
    "\n",
    "This technique involves **randomly dividing the dataset into $k$ groups/folds** of approximately equal size. The **first fold is kept for testing** and **the model is trained on $k-1$ folds**\n",
    "\n",
    "The process is repeated $K$ times and each time a different fold is used for validation.\n",
    "![Kfold ex](https://cdn-images-1.medium.com/max/1200/1*M9amI9hGx45i9k5ORS6b8w.png)\n",
    "\n",
    "As we repeat the process $k$ times, we get $k$ Mean Squared Errors (`MSE_1`,`MSE_2`,...,`MSE_K`), so KFold's CV erro is computed by taking the average of the MSE over KFolds:\n",
    "\n",
    "$$\n",
    "cv_{k} = \\frac{1}{k} \\sum^{k}_{i=1} MSE_i\n",
    "$$\n",
    "\n",
    "*Note - LOOCV is a variant of KFold where $k=1$*\n",
    "\n",
    "Typical values of $K$ in KFolds is 5 or 10. When $K$ is 10, we could call it a *10 fold cross validation*\n",
    "\n",
    "##### Advantages\n",
    "* Computation time is reduced as we repeat the process only 10 times (in the case of $K=10$)\n",
    "* Reduces bias\n",
    "* Every data point gets to be tested exactly once and is used in training $k-1$ times.\n",
    "* The variance of the resulting estimate is reduced as $k$ increases\n",
    "\n",
    "##### Disadvantages\n",
    "* The training algorithm is computationally expensive as the algorithm has to be rerun from scratch $k$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "train: [2 3], validation: [0 1]\n",
      "train: [0 1], validation: [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1,2], [3,4], [5,6], [7,8]])\n",
    "y = np.array([1,2,3,4])\n",
    "\n",
    "kf = KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "print(kf.get_n_splits(X)) # >> 2\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"train: {train_index}, validation: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified cross validation\n",
    "Stratification is a technique where we rearrange the data in a way that each fold has a good representation of the whole dataset. It forces each fold to have at least $m$ instances of each class. This approach **ensures that one class of data is not overrepresented**, especially when the target variable is unbalanced.\n",
    "\n",
    "For example, in a binary classification problem where we want to predict if a passenger on the Titanic survived or not, we ensure that each fold has a percentage of passengers that survived and a percentage of passengers that did not survive\n",
    "\n",
    "![Stratified ex](https://cdn-images-1.medium.com/max/1200/1*TuWV2i98KmBxX5qkz_gX9g.png)\n",
    "\n",
    "**Stratified cross validation helps with reducing both bias and variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series cross validation\n",
    "Splitting time series data randomly does not help because it breaks the importance of time in the data.\n",
    "\n",
    "For time series cross validation, we use **forward chaining** or **rolling-origin**. In this, each day (or time period) is a test group and we consider the previous day (or time period) as the training set.\n",
    "\n",
    "Below, `D1`, `D2`, etc are each day's data. Days in blue are used for training and days in yellow are used for testing.\n",
    "\n",
    "![time series ex](https://cdn-images-1.medium.com/max/1200/1*WMJCAkveTgbdBveMMMZtUg.png)\n",
    "\n",
    "We start training the model with a minimum number of observations and use the next day's data to test the model then keep moving through the data set. This ensures we consider the time series aspect of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0] TEST: [1]\n",
      "TRAIN: [0 1] TEST: [2]\n",
      "TRAIN: [0 1 2] TEST: [3]\n",
      "TRAIN: [0 1 2 3] TEST: [4]\n",
      "TRAIN: [0 1 2 3 4] TEST: [5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "y = np.array([1,2,3,4,5,6])\n",
    "\n",
    "tscv = TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
    "\n",
    "for train_index, test_index in tscv.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
