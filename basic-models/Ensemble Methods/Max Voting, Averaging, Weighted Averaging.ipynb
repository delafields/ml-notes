{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Techniques\n",
    "1. Max Voting\n",
    "2. Averaging\n",
    "3. Weighted Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Max Voting\n",
    "Max voting is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered a **vote**. The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "For example, say you asked 5 friends to rate a movie (out of 5) and the results were:\n",
    "\n",
    "| Friend 1  | Friend 2  | Friend 3  | Friend 4  | Friend 5  | Final Rating  |\n",
    "|---|---|---|---|---|---|\n",
    "| 5  | 4  | 5  | 4  | 4  | 4  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lgr = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "knc = KNeighborsClassifier()\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "lgr.fit(X_train, y_train)\n",
    "knc.fit(X_train, y_train)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "lgr_pred = lgr.predict(X_test)\n",
    "knc_pred = knc.predict(X_test)\n",
    "dtc_pred = dtc.predict(X_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    final_pred = np.append(final_pred, ([lgr_pred[i], knc_pred[i], dtc_pred[i]]))\n",
    "    \n",
    "# final_pred = our final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use scitkit's `VotingClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047619047619048"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model1 = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=1)\n",
    "model2 = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Averaging\n",
    "Similar to max voting, multiple predictions are made for each point in averaging. In this method, we take an average of predictions from all the models and use it to make the final predictions. Averaging can be used for making predictions in regression problems or while calculating the probabilities for classification problems.\n",
    "\n",
    "For our \"friend\" example:\n",
    "| Friend 1  | Friend 2  | Friend 3  | Friend 4  | Friend 5  | Final Rating  |\n",
    "|---|---|---|---|---|---|\n",
    "| 5  | 4  | 5  | 4  | 4  | 4.4  |\n",
    "\n",
    "$Final Prediction = \\frac{5+4+5+4+4}{5} = 4.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code\n",
    "This is the same code used as above, instead now averaging the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_average_pred = (lgr_pred + knc_pred + dtc_pred) / 3\n",
    "\n",
    "# final_average_pred = our final predictions\n",
    "# this isn't great here since we're looking for categorical responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Weighted Average\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your friends are critics while others have no prior experience, then the answers by the critics are given more importance:\n",
    "\n",
    "| _| Friend 1  | Friend 2  | Friend 3  | Friend 4  | Friend 5  | Final Rating  |\n",
    "|---|---|---|---|---|---|---|\n",
    "| weight | 0.23 | 0.23 | 0.18 | 0.18 | 0.18 |  |\n",
    "| rating | 5  | 4  | 5  | 4  | 4  |  4.41 |\n",
    "\n",
    "$$Final Rating = \\Big[(5 \\times 0.23)+(4 \\times 0.23)+(5 \\times 0.18)+(4 \\times 0.18)+(4 \\times 0.18)\\Big] = 4.41$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Code\n",
    "Again, same data from the max voting example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_wa = (lgr_pred*0.4 + dtc_pred*0.3, knc_pred*0.3)\n",
    "\n",
    "# final_pred_wa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
