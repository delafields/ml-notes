{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "The idea behind bagging is combining the results of multiple models (say, Decision Trees) to get a generalized result.\n",
    "\n",
    "If you create a bunch of models on the same set of data and combine it, will it be useful? There's a high chance that these models will all give the same result since they are getting the same input, so how can we solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping\n",
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, **with replacement**. The size of the subsets is the same as the size of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging (Bootstrap Aggregating)\n",
    "Is a technique that uses these subsets (bags) to get a fair idea of the distribution of the complete set. How it works:\n",
    "\n",
    "1. Multiple subsets are created from the original dataset, selecting observations with replacement\n",
    "![1](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/image20-768x289.png)\n",
    "\n",
    "2. A base model (weak model) is created on each of these subsets\n",
    "\n",
    "3. The models run in parallel and are independent of eachother\n",
    "\n",
    "4. The final predictions are determined by combining the predictions from all the models\n",
    "![4](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging performs best with algorithms that have high variance, like decision trees constructed without pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Implementations\n",
    "* Bagged Decision Trees\n",
    "* Random Forest\n",
    "* Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Decision Trees\n",
    "\n",
    "The example below uses scikit's `BaggingClassifier` with the Classification and Regression Trees algorithm `DecisionTreeClassifier` with 100 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "URL = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "features = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "df = pd.read_csv(URL, names=features)\n",
    "\n",
    "X = df.iloc[:, 0:8]\n",
    "y = df.iloc[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag it up\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71428571 0.83116883 0.75324675 0.63636364 0.80519481 0.83116883\n",
      " 0.80519481 0.85714286 0.71052632 0.77631579]\n",
      "0.7720608339029391\n"
     ]
    }
   ],
   "source": [
    "# Check the results\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this simple example gave us a bagged model accuracy of 77%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "A random forest is an extension of bagged decision trees.\n",
    "\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
    "\n",
    "Below is an implementation of sklearn's `RandomForestClassifier` with 100 trees and split points chosen from a random selection of 3 features. Same dataset as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "max_features = 3\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642857142857143\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "results = cross_val_score(rfc, X, y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees\n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training set. We'll construct one from sklearn's `ExtraTreesClassifier` with 100 trees and 7 random features again, using the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642173615857827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "max_features = 7\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "results = cross_val_score(etc, X, y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
