{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "Support Vector Machines (SVM) offer a powerful and flexible classifier. They're known for the kernel trick - used to handle nonlinear input spaces. They're a *discriminative classifier*, i.e. they simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divide classes from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, SVMs are used for classification problems but they can also be used in regression problems - they can easily handle multiple continuous and categorical variables. The core idea of an SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
    "![svm](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index3_souoaz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vectors**\n",
    "Support vectors are the data points which are closest to the hyperplane. These points define the separating line by calculating the margin.\n",
    "\n",
    "**Hyperplane**\n",
    "A hyperplane is a decision plane which separates a set of objects having different class memberships\n",
    "\n",
    "**Margin**\n",
    "Margin is the gap between the two lines on the closest class points - this is calculated as the perpendicular distance from the line to the support vectors. The larger the margin, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "The main objective is to segregate the given dataset via a hyperplane with the maximum possible margin between the plane and its support vectors\n",
    "\n",
    "**Steps**\n",
    "1. Generate hyperplanes which segregates the classes.\n",
    "2. Select the hyperplane with the maximum margin between support vectors\n",
    "![how svm works](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288454/index2_ub1uzd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "Some problems can't be solved using a linear hyperplane. In such a situation, SVM uses a kernel trick to transform the input space to a higher dimensional space. Post-kernel trick, the data points are plotted on the $x$ and $z$ axis. $z$ is the sum squared of both $x$ and $y$: $z=x^2+y^2$. The result looks like this:\n",
    "![kernel trick](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index_bnr4rx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Kernels\n",
    "Using the kernel trick converts a nonseparable problem into a separable problem by adding more dimension to it.\n",
    "\n",
    "* **Linear Kernel** A Linear kernel can be used by taking the dot product of any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values\n",
    "    * `K(x, xi) = sum(x * xi)`\n",
    "* **Polynomial Kernel** A polynomial kernel is a more generalized form of the linear kernel - it can distinguish curved or nonlinear input spaces\n",
    "    * `K(x, xi) = 1 + sum(x * xi)^d`\n",
    "        * Where `d` is the degree of the polynomial. `d=1` is similar to the linear transformation\n",
    "* **Radial Basis Function Kernel** The RBF kernel can map an input space in an infinitely dimensional space, making it pretty popular.\n",
    "    * `K(x, xi) = exp(-gamme * sum((x - xi^2))`\n",
    "        * Where `gamma` is a parameter that ranges from 0 to 1\n",
    "        * A higher value of gamma will perfectly fit the dataset and overfit it.\n",
    "        * `gamma=0.1` is a good default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We will look at scikit's cancer dataset - a popular multi-class classification problem. It's comprised of 30 features and a target (type of cancer), the two classes being malignant and benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "\n",
      "Labels: ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# print the features\n",
    "print(f\"Features: {cancer.feature_names}\")\n",
    "\n",
    "# print the target values\n",
    "print(f\"\\nLabels: {cancer.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (569, 30)\n",
      "\n",
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]]\n"
     ]
    }
   ],
   "source": [
    "# see the shape of the data\n",
    "print(f\"Shape: {cancer.data.shape}\\n\")\n",
    "\n",
    "# check first 5 records\n",
    "print(cancer.data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, \n",
    "                                                    cancer.target,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create the classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9649122807017544\n",
      "Precision: 0.9636363636363636\n",
      "Recall:    0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "* **Kernel:** The main function of the kernel is to transform the given dataset to the required form - this involves changing the kernel function.\n",
    "    * Polynomial and RBF are useful for non-linear hyperplanes.\n",
    "    * In some applications, a more complex kernel is necessary.\n",
    "* **Regularization:** The regularization parameter `C` is the penalty parameter which represents the misclassification or error term. This parameter tells the SVM optimizer how much error is bearable - it's how you control the tradeoff between decision boundary and missclassification term.\n",
    "    * A smaller value of `C` creates a small-margin hyperplane\n",
    "    * A larger value of `C` creates a larger-margin hyperplane\n",
    "* **Gamma:** A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, causing overfitting.\n",
    "    * A low value of gamma considers only nearby points when calculating the separation line\n",
    "    * A high value of gamma considers all of the data points when calculating the separation line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "* Low memory use (they use a subset of training points in the decision phase)\n",
    "* Fast predictions. \n",
    "* Works well with a clear margin of separation and with high dimensional space\n",
    "* Versatile, due to the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "* Not suitable for large datasets because of its high training time\n",
    "* Works poorly with overlapping classes - tuning the softening parameter $c$ is very important\n",
    "* Sensitive to the type of kernel used\n",
    "* The results do not have a probabalistic interpretation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
