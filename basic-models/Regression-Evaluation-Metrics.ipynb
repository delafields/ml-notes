{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Regression deals with the modelling of **continuous values**. Linear regression creates a model that assumes a linear relationship between the inputs and outputs i.e. - the higher the inputs the higher (or lower) the outputs are.\n",
    "\n",
    "What adjusts how *strong* the relationship is and the *direction* of this relationship is are our **coefficients**. Our first coefficient without an input is the **intercept**. Our goal is to calculate the optimal coefficients.\n",
    "\n",
    "Univariate lin-reg comes in the form\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\hat{y}$: Predicted output\n",
    "* $\\beta_0$ and $\\beta_1$: Coefficients\n",
    "* $x$: input\n",
    "* $\\epsilon$: error term\n",
    "\n",
    "### Comparing predictions against reality\n",
    "\n",
    "We use **residuals** to calculate model accuracy. **Residuals** are the differences between the actual values and our predicted values.\n",
    "\n",
    "Now, we don't inspect all of the residuals as is - we typically use some type of **summary statistic** to evaluate the predictive power of our model.\n",
    "\n",
    "To illustrate some of these methods, we'll load the boston housing dataset from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(['ggplot'])\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "bos = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "bos['PRICE'] = boston.target\n",
    "\n",
    "X = bos.drop('PRICE', axis=1)\n",
    "y = bos['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Squared\n",
    "Also known as the *coefficient of determination* his is pretty much the default scoring method for regression techniques.\n",
    "\n",
    "This metric indicates how well model predictions approximates the true values where 1 indicates a perfect fit and 0 would be a regressor that always predicts the mean of the data.\n",
    "\n",
    "**What's good**: it has an intuitive scale that doesn't depend on the units of the target variable.\n",
    "\n",
    "**What's bad**: it says nothing about hte prediction error of the model (which is quite important).\n",
    "\n",
    "#### Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694140401906062"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=100)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "We calculate MAE by taking the absolute value of the residual for every data point, so that negative and positive residuals do not cancel out. We then take the average of all these residuals.\n",
    "\n",
    "A small MAE suggests the model is great at prediction, while a large MAE suggests the model may have trouble in certain areas.\n",
    "\n",
    "Effectively, MAE describes the *typical* magnitude of the residuals. Because we use the the absolute value of the residual, the MAE does not indicate **underperformance** or **overperformance** of the model. Each residual contributes proportionally to the total amount of error, meaning that larger errors will contribute linearly to the overall error.\n",
    "\n",
    "While MAE is easily interpretable, using the absolute value of the residual is often not as desirable as **squaring** this difference. Depending on how you want your model to treat **outliers**, you may want to bring more attention to these.\n",
    "\n",
    "Equation:\n",
    "$$\n",
    "MAE\\; = \\; \\frac{1}{n} \\sum \\vert \\; y - \\hat{y} \\; \\vert\n",
    "$$\n",
    "\n",
    "Using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.17978701672763"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "MSE is just like the MAE, but it *squares* the difference before summing them all instead of using the absolute value.\n",
    "\n",
    "**Consequences of the Square Term**\n",
    "Because we are squaring the difference, MSE will almost always be bigger than MAE (meaning we can't directly compare the two). The effect of the square term is most apparent with the **presence of outliers in our data**. While each residual in MAE contributes **proportionally** to the total error, the error grows **quadratically** in MSE. This ultimately means that outliers in our data will contribute to much higher total error in the MSE than they would in the MAE. In turn, this means our model would be penalized more for making predictions that differ greatly from the corresponding actual value.\n",
    "\n",
    "**The choice between MAE and MSE really comes down to how important we consider outliers**\n",
    "\n",
    "Equation:\n",
    "$$\n",
    "MSE\\; = \\; \\frac{1}{n} \\sum ( \\; y - \\hat{y} \\; )^2\n",
    "$$\n",
    "\n",
    "Using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.273296310266296"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is just the square root of the MSE. Because the MSE is squared, its units do not match that of the original output. RMSE is used in order to convert the error metric back into similar units, making interpretation easier. *They're both similarly affected by outliers**\n",
    "\n",
    "The RMSE is analogous to the standard deviation (MSE to variance) and is a measure of how large your residuals are spread out.\n",
    "\n",
    "Both MAE and MSE can range from 0 to positive infinity, so as both of these measurements get higher, it becomes harder to interpret model performance. Another way we can summarize our collection of residuals is by using percentages so that each prediction is scaled against the value it's supposed to estimate.\n",
    "\n",
    "Using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.719459323933865"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
